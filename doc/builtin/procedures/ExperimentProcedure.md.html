<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8' />
<title>MLDB Documentation</title>
<script src='/resources/js/mathjax_TeX-AMS-MML_HTMLorMML.js'></script>
<link rel='stylesheet' href='/resources/css/prism.css'>
<link rel='stylesheet' href='/resources/css/doc.css'>
<script src='/resources/js/jquery-1.11.2.min.js'></script>
<script src='/resources/js/prism.js'></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  if(window.location.origin == 'http://mldb.ai')
  { ga('create', 'UA-16909325-9', 'auto'); }
  else
  { ga('create', 'UA-16909325-10', {'cookieDomain': 'none'}); }
  ga('send', 'pageview');
</script>
</head>
<body style='margin-left: 50px; max-width: 1000px'>
<h1>Classifier Experiment Procedure</h1>

<p>The classifier experiment procedure is used to train and evaluate a classifier in a single run.
It wraps the <a href="/doc/builtin/procedures/Classifier.md.html"><code>classifier.train</code> procedure type</a> and the <a href="/doc/builtin/procedures/Accuracy.md.html"><code>classifier.test</code> procedure type</a> into a single and easier to use procedure. It can also perform k-fold cross-validation by specifying multiple folds over the data to use for training and testing.</p>

<p>The <code>classifier.experiment</code> procedure will run multiple rounds of training and testing, based on the settings of the <code>inputData</code>, <code>kfold</code>, <code>datasetFolds</code> and <code>testingDataOverride</code> parameters, according to the <a href="#TrainTest">steps laid out below</a>.</p>

<h2>Configuration</h2>

<p><p>A new procedure of type <code>classifier.experiment</code> named <code>&lt;id&gt;</code> can be created as follows:</p><pre><code class="language-python">mldb.put(&quot;/v1/procedures/&quot;+&lt;id&gt;, {
    &quot;type&quot;: &quot;classifier.experiment&quot;,
    &quot;params&quot;: {
        "experimentName": &lt;string&gt;,
        "mode": &lt;ClassifierMode&gt;,
        "multilabelStrategy": &lt;MultilabelStrategy&gt;,
        "recallOverN": &lt;ARRAY [ int ]&gt;,
        "inputData": &lt;<a href="/doc/builtin/procedures/InputQuery.md.html">InputQuery</a>&gt;,
        "kfold": &lt;int&gt;,
        "datasetFolds": &lt;ARRAY [ <a href="/doc/builtin/procedures/ExperimentProcedure.md.html#DatasetFoldConfig">DatasetFoldConfig</a> ]&gt;,
        "testingDataOverride": &lt;<a href="/doc/builtin/procedures/InputQuery.md.html">InputQuery</a> (Optional)&gt;,
        "algorithm": &lt;string&gt;,
        "configuration": &lt;JSON&gt;,
        "configurationFile": &lt;string&gt;,
        "equalizationFactor": &lt;float&gt;,
        "modelFileUrlPattern": &lt;<a href="/doc/builtin/Url.md.html">Url</a>&gt;,
        "keepArtifacts": &lt;bool&gt;,
        "evalTrain": &lt;bool&gt;,
        "outputAccuracyDataset": &lt;bool&gt;,
        "uniqueScoresOnly": &lt;bool&gt;,
        "runOnCreation": &lt;bool&gt;
    }
})</code></pre><p>with the following key-value definitions for <code>params</code>:</p><table class="params table" width='100%'><tr><th align='right'>Field, Type, Default</th><th>Description</th></tr>
<tr><td align='right'><p><strong>experimentName</strong> <br/> <nobr>string</nobr> <br/> <code></code></p></td><td><p>A string without spaces which will be used to name the various datasets, procedures and functions created this procedure runs.</p>
</td></tr>
<tr><td align='right'><p><strong>mode</strong> <br/> <nobr>ClassifierMode</nobr> <br/> <code>"boolean"</code></p></td><td><p>Model mode: <code>boolean</code>, <code>regression</code> or <code>categorical</code>. Controls how the label is interpreted and what is the output of the classifier. </p>
</td></tr>
<tr><td align='right'><p><strong>multilabelStrategy</strong> <br/> <nobr>MultilabelStrategy</nobr> <br/> <code>"one-vs-all"</code></p></td><td><p>Multilabel strategy: <code>random</code>, <code>decompose</code> or <code>onevsall</code>Controls how examples are prepared to handle multilabel classification.  *<code>random</code> will select a label at random among the example&#39;s set  *<code>decompose</code> will decompose the multilabel combination into single-label examples  *<code>onevsall</code> will train a probabilized binary classifier for each labelThis only applies if <code>mode</code> is equal to <code>multilabel</code></p>
</td></tr>
<tr><td align='right'><p><strong>recallOverN</strong> <br/> <nobr>ARRAY [ int ]</nobr> <br/> <code></code></p></td><td><p>Calculate a recall score over the top scoring labels.Does not apply to boolean or regression modes.</p>
</td></tr>
<tr><td align='right'><p><strong>inputData</strong> <br/> <nobr><a href="/doc/builtin/procedures/InputQuery.md.html">InputQuery</a></nobr> <br/> <code></code></p></td><td><p>SQL query which specifies the features, labels and optional weights for the training and testing procedures. This query is used to create a training and testing set according to the <a href="#TrainTest">steps laid out below</a>.</p>

<p>The query should be of the form <code>select {f1, f2} as features, x as label from ds</code>.</p>

<p>The select expression must contain these two columns: </p>

<ul>
<li><code>features</code>: a row expression to identify the features on which to 
train, and </li>
<li><code>label</code>: one expression to identify the row&#39;s label(s), and whose type must match that of the classifier mode. Rows with null labels will be ignored. 

<ul>
<li><code>boolean</code> mode: a boolean (0 or 1)</li>
<li><code>regression</code> mode: a real number</li>
<li><code>categorical</code> mode: any combination of numbers and strings</li>
<li><code>multilabel</code> mode: a row, in which each non-null column is a separate label</li>
</ul></li>
</ul>

<p>The select expression can contain an optional <code>weight</code> column. The weight allows the relative importance of examples to be set. It must be a real number. If the <code>weight</code> is not specified each row will have a weight of 1. Rows with a null weight will cause a training error. </p>

<p>The query must not contain <code>WHERE</code>, <code>LIMIT</code>, <code>OFFSET</code>, <code>GROUP BY</code> or <code>HAVING</code> clauses (they can be defined in datasetFolds) and, unlike most select expressions, this one can only select whole columns, not expressions involving columns. So <code>X</code> will work, but not <code>X + 1</code>. If you need derived values in the query, create a dataset with the derived columns as a previous step and use a query on that dataset instead.</p>
</td></tr>
<tr><td align='right'><p><strong>kfold</strong> <br/> <nobr>int</nobr> <br/> <code>0</code></p></td><td><p>Do a k-fold cross-validation. This is a helper parameter that generates a <a href="#DatasetFoldConfig">DatasetFoldConfig</a> splitting the dataset into k subsamples, each fold testing on one <code>k</code>th of the input data and training on the rest.</p>

<p>0 means it is not used and 1 is an invalid number. It cannot be specified at the same time as the <code>datasetFolds</code> parameter and in general should not be used at the same time as the <code>testingDataOverride</code> parameter.</p>
</td></tr>
<tr><td align='right'><p><strong>datasetFolds</strong> <br/> <nobr>ARRAY [ <a href="/doc/builtin/procedures/ExperimentProcedure.md.html#DatasetFoldConfig">DatasetFoldConfig</a> ]</nobr> <br/> <code></code></p></td><td><p><a href="#DatasetFoldConfig">DatasetFoldConfig</a> to use. This parameter can be used if the dataset folds required are more complex than a simple k-fold cross-validation. It cannot be specified as the same time as the <code>kfold</code> parameter and in general should not be used at the same time as the <code>testingDataOverride</code> parameter.</p>
</td></tr>
<tr><td align='right'><p><strong>testingDataOverride</strong> <br/> <nobr><a href="/doc/builtin/procedures/InputQuery.md.html">InputQuery</a> (Optional)</nobr> <br/> <code></code></p></td><td><p>SQL query which overrides the input data for the testing procedure. This optional parameter must be of the same form as the <code>inputData</code> parameter above, and by default takes the same value as <code>inputData</code>.</p>

<p>This query is used to create a test set according to the <a href="#TrainTest">steps laid out below</a>.</p>

<p>This parameter is useful when it is necessary to test on data contained in a different dataset from the training data, or to calculate accuracy statistics with uneven weighting, for example to counteract the effect of non-uniform sampling in the <code>inputData</code>.</p>

<p>The query must not contain <code>WHERE</code>, <code>LIMIT</code>, <code>OFFSET</code>, <code>GROUP BY</code> or <code>HAVING</code> clauses (they can be defined in datasetFolds) and, unlike most select expressions, this one can only select whole columns, not expressions involving columns. So <code>X</code> will work, but not <code>X + 1</code>. If you need derived values in the query, create a dataset with the derived columns as a previous step and use a query on that dataset instead.</p>
</td></tr>
<tr><td align='right'><p><strong>algorithm</strong> <br/> <nobr>string</nobr> <br/> <code></code></p></td><td><p>Algorithm to use to train classifier with.  This must point to an entry in the configuration or configurationFile parameters. See the <a href="../ClassifierConf.md.html">classifier configuration documentation</a> for details.</p>
</td></tr>
<tr><td align='right'><p><strong>configuration</strong> <br/> <nobr>JSON</nobr> <br/> <code></code></p></td><td><p>Configuration object to use for the classifier.  Each one has its own parameters.  If none is passed, then the configuration will be loaded from the ConfigurationFile parameter. See the <a href="../ClassifierConf.md.html">classifier configuration documentation</a> for details.</p>
</td></tr>
<tr><td align='right'><p><strong>configurationFile</strong> <br/> <nobr>string</nobr> <br/> <code>"/opt/bin/classifiers.json"</code></p></td><td><p>File to load configuration from.  This is a JSON file containing only objects, strings and numbers.  If the configuration object is non-empty, then that will be used preferentially. See the <a href="../ClassifierConf.md.html">classifier configuration documentation</a> for details.</p>
</td></tr>
<tr><td align='right'><p><strong>equalizationFactor</strong> <br/> <nobr>float</nobr> <br/> <code>0.5</code></p></td><td><p>Amount to adjust weights so that all classes have an equal total weight.  A value of 0 (default) will not equalize weights at all.  A value of 1 will ensure that the total weight for both positive and negative examples is exactly identical. A number between will choose a balanced tradeoff.  Typically 0.5 is a good number to use for unbalanced probabilities. See the <a href="../ClassifierConf.md.html">classifier configuration documentation</a> for details.</p>
</td></tr>
<tr><td align='right'><p><strong>modelFileUrlPattern</strong> <br/> <nobr><a href="/doc/builtin/Url.md.html">Url</a></nobr> <br/> <code></code></p></td><td><p>URL where the model file (with extension &#39;.cls&#39;) should be saved. It should include the string $runid that will be replaced by an identifier for each run, if using multiple dataset folds.</p>
</td></tr>
<tr><td align='right'><p><strong>keepArtifacts</strong> <br/> <nobr>bool</nobr> <br/> <code>false</code></p></td><td><p>If true, all procedures and intermediary datasets are kept.</p>
</td></tr>
<tr><td align='right'><p><strong>evalTrain</strong> <br/> <nobr>bool</nobr> <br/> <code>false</code></p></td><td><p>Run the evaluation on the training set. If true, the same performance statistics that are returned for the testing set will also be returned for the training set.</p>
</td></tr>
<tr><td align='right'><p><strong>outputAccuracyDataset</strong> <br/> <nobr>bool</nobr> <br/> <code>true</code></p></td><td><p>If true, an output dataset for scored examples will created for each fold.</p>
</td></tr>
<tr><td align='right'><p><strong>uniqueScoresOnly</strong> <br/> <nobr>bool</nobr> <br/> <code>false</code></p></td><td><p>If <code>outputAccuracyDataset</code> is set and <code>mode</code> is set to <code>boolean</code>, setting this parameter to <code>true</code> will output a single row per unique score. This is useful if the test set is very large and aggregate statistics for each unique score is sufficient, for instance to generate a ROC curve. This has no effect for other values of <code>mode</code>.</p>
</td></tr>
<tr><td align='right'><p><strong>runOnCreation</strong> <br/> <nobr>bool</nobr> <br/> <code>true</code></p></td><td><p>If true, the procedure will be run immediately. The response will contain an extra field called <code>firstRun</code> pointing to the URL of the run.</p>
</td></tr>
</table></p>

<h3>Algorithm configuration</h3>

<p>This procedures supports many training algorithm. The configuration is explained
on the <a href="../ClassifierConf.md.html">classifier configuration</a> page.</p>

<p><a name="DatasetFoldConfig"></a></p>

<h2>Dataset Folds and Cross-validation</h2>

<p>The experiment procedure supports k-fold 
<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_blank">cross-validation</a> in a
flexible way. Folds can be specified implicitly using the <code>kfolds</code> parameter, or passing in a DatasetFoldConfig to the <code>datasetFolds</code> parameter as follows:</p>

<h3>DatasetFoldConfig</h3>

<p><table class="params table" width='100%'><tr><th align='right'>Field, Type, Default</th><th>Description</th></tr>
<tr><td align='right'><p><strong>trainingWhere</strong> <br/> <nobr><a href="/doc/builtin/sql/ValueExpression.md.html">string</a></nobr> <br/> <code>"true"</code></p></td><td><p>The WHERE clause for which rows to include from the training dataset. This can be any expression involving the columns in the dataset. </p>
</td></tr>
<tr><td align='right'><p><strong>testingWhere</strong> <br/> <nobr><a href="/doc/builtin/sql/ValueExpression.md.html">string</a></nobr> <br/> <code>"true"</code></p></td><td><p>The WHERE clause for which rows to include from the testing dataset. This can be any expression involving the columns in the dataset. </p>
</td></tr>
<tr><td align='right'><p><strong>trainingOffset</strong> <br/> <nobr>int</nobr> <br/> <code>0</code></p></td><td><p>How many rows to skip before using data.</p>
</td></tr>
<tr><td align='right'><p><strong>trainingLimit</strong> <br/> <nobr>int</nobr> <br/> <code>-1</code></p></td><td><p>How many rows of data to use.  -1 (the default) means use all of the rest of the rows in the dataset after skipping OFFSET rows.</p>
</td></tr>
<tr><td align='right'><p><strong>testingOffset</strong> <br/> <nobr>int</nobr> <br/> <code>0</code></p></td><td><p>How many rows to skip before using data.</p>
</td></tr>
<tr><td align='right'><p><strong>testingLimit</strong> <br/> <nobr>int</nobr> <br/> <code>-1</code></p></td><td><p>How many rows of data to use.  -1 (the default) means use all of the rest of the rows in the dataset after skipping OFFSET rows.</p>
</td></tr>
<tr><td align='right'><p><strong>trainingOrderBy</strong> <br/> <nobr><a href="/doc/builtin/sql/OrderByExpression.md.html">SqlOrderByExpression</a></nobr> <br/> <code>"rowHash()"</code></p></td><td><p>How to order the rows.  This only has an effect when <code>trainingOffset</code> or <code>trainingLimit</code> are used.</p>
</td></tr>
<tr><td align='right'><p><strong>testingOrderBy</strong> <br/> <nobr><a href="/doc/builtin/sql/OrderByExpression.md.html">SqlOrderByExpression</a></nobr> <br/> <code>"rowHash()"</code></p></td><td><p>How to order the rows.  This only has an effect when <code>testingOffset</code> or <code>testingLimit</code> are used.</p>
</td></tr>
</table></p>

<h3>Example: 3-fold cross-validation</h3>

<p>To perform a 3-fold cross-validation, you can set the parameter <code>kfold</code> to 3 or <em>equivalently</em>, set the <code>datasetFolds</code> parameter to the following value:</p>

<pre><code class="language-python">[
    {
        &quot;trainingWhere&quot;: &quot;rowHash() % 3 != 0&quot;,
        &quot;testingWhere&quot;: &quot;rowHash() % 3 = 0&quot;,
    },
    {
        &quot;trainingWhere&quot;: &quot;rowHash() % 3 != 1&quot;,
        &quot;testingWhere&quot;: &quot;rowHash() % 3 = 1&quot;,
    },
    {
        &quot;trainingWhere&quot;: &quot;rowHash() % 3 != 2&quot;,
        &quot;testingWhere&quot;: &quot;rowHash() % 3 = 2&quot;,
    }
]
</code></pre>

<p><a name="TrainTest"></a></p>

<h2>Training and Testing Set Generation</h2>

<p>The <code>classifier.experiment</code> procedure will run multiple rounds of training and testing, based on the settings of the <code>inputData</code>, <code>kfold</code>, <code>datasetFolds</code> and <code>testingDataOverride</code> parameters.</p>

<p>The procedure first assembles a DatasetFoldConfig using the following rules:</p>

<ol>
<li>if <code>datasetFolds</code> is specified, that is the DatasetFoldConfig</li>
<li>else if <code>kfold</code> is specified, a DatasetFoldConfig is generated as per the example above</li>
<li>else if <code>testingDataOverride</code> is specified, the DatasetFoldConfig is <code>[{&quot;trainingWhere&quot;: &quot;true&quot;, &quot;testingWhere&quot;: &quot;true&quot;}]</code> (i.e. the procedure will train on <code>inputData</code> and test on <code>testingDataOverride</code>)</li>
<li>else (i.e. <strong>base case</strong> of only <code>inputData</code> specified) the DatasetFoldConfig is <code>[{&quot;trainingWhere&quot;: &quot;rowHash() % 2 != 1&quot;, &quot;testingWhere&quot;: &quot;rowHash() % 2 = 1&quot;}]</code> (i.e. the procedure will train on half of <code>inputData</code> and test on the other half)</li>
</ol>

<p>The procedure then assembles training and testing sets for each entry in the DatasetFoldConfig:</p>

<ol>
<li>The training set will be the result of the query built by combining <code>inputData</code> with the <code>trainingWhere</code>, <code>trainingOffset</code>, <code>trainingLimit</code> and <code>orderBy</code> parameters of the DatasetFoldConfig entry</li>
<li>The testing query will be the result of the query built by combining  <code>inputData</code> (or <code>testingDataOverride</code> if specified) with the <code>testingWhere</code>, <code>testingOffset</code>, <code>testingLimit</code> and <code>orderBy</code> parameters of the DatasetFoldConfig entry. The procedure will automatically use the <code>classifier</code> function generated by the training and call it with the features in the testing query to generate a score to compare to the label.</li>
</ol>

<h2>Output</h2>

<p>The output will contain performance metrics over each fold that was tested. See the 
<a href="/doc/builtin/procedures/Accuracy.md.html"><code>classifier.test</code> procedure type</a> page for a sample output.</p>

<p>An aggregated version of the metrics over all folds is also provided. The aggregated
results blob will have the same structure as each fold but every numeric metric will
be replaced by an object containing standard statistics (min, max, mean, std).
In the example below, the <code>auc</code> metric is used to illustrate the aggregation.</p>

<p>The following example would be for a 2-fold run:</p>

<pre><code class="language-python">{
    &quot;id&quot; : &quot;&lt;id&gt;&quot;,
    &quot;runFinished&quot; : &quot;...&quot;,
    &quot;runStarted&quot; : &quot;...&quot;,
    &quot;state&quot; : &quot;finished&quot;,
    &quot;status&quot; : {
        &quot;aggregatedTest&quot;: {
            &quot;auc&quot;: {
                &quot;max&quot;: x,
                &quot;mean&quot;: y,
                &quot;min&quot;: z,
                &quot;std&quot;: k
            },
            ...
        },
        &quot;folds&quot;: [
            {
                &quot;accuracyDataset&quot; : &lt;id of fold 1 accuracy dataset if it was generated&gt;,
                &quot;modelFileUrl&quot; = &lt;path of fold 1 model file&gt;,
                &quot;functionName&quot; = &lt;name of fold 1 scorer function&gt;,
                &quot;resultsTest&quot; : { &lt;classifier.test output for fold 1&gt; },
                &quot;fold&quot;: { &lt;datasetFold used for fold 1&gt; }
            },
            {
                &quot;accuracyDataset&quot; : &lt;id of fold 2 accuracy dataset if it was generated&gt;,
                &quot;modelFileUrl&quot; = &lt;path of fold 2 model file&gt;,
                &quot;functionName&quot; = &lt;name of fold 2 scorer function&gt;,
                &quot;resultsTest&quot; : { &lt;classifier.test output for fold 2&gt; },
                &quot;fold&quot;: { &lt;datasetFold used for fold 2&gt; }
            }
        ]
    }
}
</code></pre>

<p>If the training set is also evaluated, the additional keys <code>aggregatedTrain</code> and <code>resultsTrain</code> 
will also be returned and will have the same structure as the results for the testing set.</p>

<h2>Examples</h2>

<ul>
<li>The <a href="/doc/nblink.html#_demos/Predicting%20Titanic%20Survival" target="_blank">Predicting Titanic Survival</a> demo notebook</li>
</ul>

<h2>See also</h2>

<ul>
<li><a href="/doc/builtin/procedures/Classifier.md.html"><code>classifier.train</code> procedure type</a></li>
<li><a href="/doc/builtin/procedures/Accuracy.md.html"><code>classifier.test</code> procedure type</a></li>
<li><a href="/doc/builtin/functions/ClassifierApply.md.html"><code>classifier</code> function type</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_blank">Cross-validation</a> on Wikipedia</li>
</ul>
</body>
</html>
