<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8' />
<title>MLDB Documentation</title>
<script src='/resources/js/mathjax_TeX-AMS-MML_HTMLorMML.js'></script>
<link rel='stylesheet' href='/resources/css/prism.css'>
<link rel='stylesheet' href='/resources/css/doc.css'>
<script src='/resources/js/jquery-1.11.2.min.js'></script>
<script src='/resources/js/prism.js'></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  if(window.location.origin == 'http://mldb.ai')
  { ga('create', 'UA-16909325-9', 'auto'); }
  else
  { ga('create', 'UA-16909325-10', {'cookieDomain': 'none'}); }
  ga('send', 'pageview');
</script>
</head>
<body style='margin-left: 50px; max-width: 1000px'>
<h1>TF-IDF Function</h1>

<p>The TF-IDF function is used to find how relevant certain words are to a document, by 
combining the term frequency (TF), i.e how frequent the term is in the document, with the inverse document frequency (IDF), i.e how frequent a term
appears in a reference corpus.</p>

<h2>Configuration</h2>

<p><p>A new function of type <code>tfidf</code> named <code>&lt;id&gt;</code> can be created as follows:</p><pre><code class="language-python">mldb.put(&quot;/v1/functions/&quot;+&lt;id&gt;, {
    &quot;type&quot;: &quot;tfidf&quot;,
    &quot;params&quot;: {
        "modelFileUrl": &lt;<a href="/doc/builtin/Url.md.html">Url</a>&gt;,
        "tfType": &lt;TFType&gt;,
        "idfType": &lt;IDFType&gt;
    }
})</code></pre><p>with the following key-value definitions for <code>params</code>:</p><table class="params table" width='100%'><tr><th align='right'>Field, Type, Default</th><th>Description</th></tr>
<tr><td align='right'><p><strong>modelFileUrl</strong> <br/> <nobr><a href="/doc/builtin/Url.md.html">Url</a></nobr> <br/> <code></code></p></td><td><p>URL of the model file (with extension &#39;.idf&#39;) to load. This file is created by the <a href="/doc/builtin/procedures/TfidfProcedure.md.html"><code>tfidf.train</code> procedure type</a>.</p>
</td></tr>
<tr><td align='right'><p><strong>tfType</strong> <br/> <nobr>TFType</nobr> <br/> <code>"raw"</code></p></td><td><p>Type of TF scoring</p>
</td></tr>
<tr><td align='right'><p><strong>idfType</strong> <br/> <nobr>IDFType</nobr> <br/> <code>"inverseSmooth"</code></p></td><td><p>Type of IDF scoring</p>
</td></tr>
</table></p>

<h3>Type of TF scoring</h3>

<p>Given a term \( t \), \( D \) the corpus, a document \( d \in D \) and the term frequency denoted by \(f_{t,d}\), 
the type of TF weighting schemes are:</p>

<table>
<thead>
<tr>
<th>weighting scheme</th>
<th>description</th>
<th>TF weight</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>raw</code></td>
<td>the term frequency in the document</td>
<td>\(f_{t,d}\)</td>
</tr>
<tr>
<td><code>log</code></td>
<td>the logarithm of the term frequency in the document</td>
<td>\( \log(1 + f_{t,d}) \)</td>
</tr>
<tr>
<td><code>augmented</code></td>
<td>the half term frequency in the document divided by the maximum frequency of any term in the document</td>
<td>\( 0.5 + 0.5 \cdot \frac { f_{t,d} }{\max_{\{t&#39; \in d\}} {f_{t&#39;,d}}} \)</td>
</tr>
</tbody>
</table>

<h3>Type of IDF scoring</h3>

<p>Given:</p>

<ul>
<li>\( N \): the total number of documents in the corpus (\( N = |D| \))</li>
<li>\( n_t = |\{d \in D: t \in d\}| \): number of documents where the term \( t \) appears in</li>
</ul>

<p>The types of IDF weighting schemes are:</p>

<table>
<thead>
<tr>
<th>weighting scheme</th>
<th>description</th>
<th>IDF weight</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>unary</code></td>
<td>unary IDF score, i.e. don&#39;t use IDF</td>
<td>\( 1 \)</td>
</tr>
<tr>
<td><code>inverse</code></td>
<td>the logarithm of the number of documents in the corpus divided by the number of documents the term appears in (this will lead to negative scores for terms appearing in all documents in the corpus)</td>
<td>\( \log \left( \frac {N}{1 + n_t } \right) \)</td>
</tr>
<tr>
<td><code>inverseSmooth</code></td>
<td>similar to <code>inverse</code> but with 1 added to the logarithmic term (this ensures that the score will never be negative)</td>
<td>\( \log \left( 1 + \frac {N}{1 + n_t } \right) \)</td>
</tr>
<tr>
<td><code>inverseMax</code></td>
<td>similar to <code>inverseSmooth</code> but using the maximum term frequency</td>
<td>\( \log \left(1 + \frac {\max_{\{t&#39; \in d\}} n_{t&#39;}} {1 + n_t}\right)  \)</td>
</tr>
<tr>
<td><code>probabilisticInverse</code></td>
<td>similar to <code>inverse</code> but substracting the number of documents the term appears in from the total number of documents in the training corpus (this can lead to positive and negative scores)</td>
<td>\(  \log \left( \frac {N - n_t} {1 + n_t} \right) \)</td>
</tr>
</tbody>
</table>

<h2>Input and Output Values</h2>

<p>The function takes a single input named <code>input</code> that contains the list of words to be evaluated, with the column name being the term, and the value being the number of time the word is present in the document. This can be prepared using the tokenize function or any other method.</p>

<p>The function returns a single output named <code>output</code> that contains the combined TF-IDF score for each term in the input.</p>

<h2>See also</h2>

<ul>
<li>The <a href="/doc/builtin/procedures/TfidfProcedure.md.html"><code>tfidf.train</code> procedure type</a> trains the data to use in this function.</li>
<li><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank">tf-idf on Wikipedia</a></li>
</ul>
</body>
</html>
